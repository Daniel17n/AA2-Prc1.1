{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTFapQRnBLhd"
      },
      "source": [
        "### Imports\n",
        "\n",
        "#### Necesitaremos las siguientes librerías instaladas en nuestro entorno de Python:\n",
        "- **Torch** : Por motivos obvios, además crearemos algunos alias para acortar líneas.\n",
        "- **Pandas** : Para crear nuestro dataframework.\n",
        "- **Numpy** : Para trabajar con los arrays y vectores de una manera más eficiente.\n",
        "- **opencv-python** : Librería para cargar las imágenes y redimensionarlas (CV2).\n",
        "- **MatPlotLib** : Grafica el _loss_ en entrenamiento y en validación.\n",
        "- **SKLearn** : Por su función _train_test_split()_.\n",
        "\n",
        "Para ello, ejecutaremos los siguientes comandos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4B6el_7CHNrh",
        "outputId": "58f2664d-a1e7-45db-c399-e0d74debc77a"
      },
      "outputs": [],
      "source": [
        "%pip install torch\n",
        "%pip install pandas\n",
        "%pip install numpy\n",
        "%pip install opencv-python\n",
        "%pip install matplotlib\n",
        "%pip install scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpGVDFRV70jM"
      },
      "source": [
        "Una vez descargadas las librerías, realizamos el import de todas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j50g-cHsBNet"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.model_selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUQObQL9BwKW"
      },
      "source": [
        "## Configuración\n",
        "Se declaran las siguientes variables estáticas para usarlas después en el código:\n",
        "\n",
        "* ***DEVICE***  : Establece el dispositivo que se utilizará para realizar el    entrenamiento. En caso de que el sistema disponga de una tarjeta gráfica con CUDA, se utilizará ésta. En caso contrario, se utilizará la CPU.\n",
        "* ***IMG_SIZE*** : Especifica el tamaño al que se establecerán las imágenes\n",
        "* ***BATCH_SIZE*** : Tamaño de los batches que se usarán durante la carga de datos\n",
        "* ***EPOCHS*** : Se especifica la cantidad de epochs a realizar durante la ejecución del código.\n",
        "* ***LEARNING_RATE*** : Establece la velocidad de aprendizaje del modelo; Se utiliza un valor por defecto de 0.001."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhYzQbqvBh9X"
      },
      "outputs": [],
      "source": [
        "# Configuración\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "IMG_SIZE = 64\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 200\n",
        "LEARNING_RATE = 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_lawvdF77u-"
      },
      "source": [
        "Para poder diferenciar bien que hace cada parte de código, creamos una clase con el Dataset de las flechas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOi6aY-4FzAY"
      },
      "outputs": [],
      "source": [
        "# Dataset personalizado\n",
        "class ArrowDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.data = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.data.iloc[idx]['image_path']\n",
        "        angle = self.data.iloc[idx]['angle']\n",
        "\n",
        "        # Leer imagen\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "        img = img.astype(np.float32) / 255.0  # Normalización [0, 1]\n",
        "        img = np.expand_dims(img, axis=0)  # Añadir canal\n",
        "\n",
        "        # Normalizar ángulo entre 0 y 1\n",
        "        angle_normalized = angle\n",
        "\n",
        "        return torch.tensor(img, dtype=torch.float32), torch.tensor(angle_normalized, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definimos el modelo convolucional como una serie de capas de convolución, seguidas de una capa de activación ReLU y una capa de MaxPool de 2x2, para reducir el tamaño de la(s) imágenes resultantes. A cada capa que se suma, se duplican el número de filtros de la capa anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pdx76dAqGAQG"
      },
      "outputs": [],
      "source": [
        "# Modelo CNN\n",
        "class ArrowAngleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ArrowAngleCNN, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * 8 * 8, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 1), nn.Sigmoid()  # Salida normalizada entre 0 y 1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = self.fc_layers(x)\n",
        "        return x.squeeze()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3iL7Yoq_bWs"
      },
      "source": [
        "Esta sección sería lo que sucede antes de la ejecución principal de cualquier rutina. En ella se crea el dataframework a partir de csv y se separan el conjunto de entrenamiento y de validación.\n",
        "\n",
        "Se crean los DataLoaders para entrenamiento y para validación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "PwCvf853GE6V",
        "outputId": "7a7927b5-fe6e-4608-ff45-6b1cfc0a65da"
      },
      "outputs": [],
      "source": [
        "# Cargar datos del archivo CSV (separando por tabulador)\n",
        "df = pd.read_csv('dataset_cleaned.csv', sep='\\t', header=None, names=['image_path', 'angle'])\n",
        "train_df, val_df = sklearn.model_selection.train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = ArrowDataset(train_df)\n",
        "val_dataset = ArrowDataset(val_df)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLHCKdZh_2AF"
      },
      "source": [
        "El corazón de todo esto, el meollo.\n",
        "Aquí se manda el modelo al dispositivo (normalmente CUDA) y se crea el optimizador, en nuestro caso usamos ADAM.\n",
        "\n",
        "Creamos el 'alias' de la función MSELoss() e iteramos sobre la cantidad de epochs a realizar.\n",
        "\n",
        "El programa va a ir imprimiendo para cada epoch: el epoch actual, el RMSELoss del modelo en entrenamiento y el RMSELoss del modelo en validación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIm1lAInGXOZ"
      },
      "outputs": [],
      "source": [
        "# Entrenamiento\n",
        "model = ArrowAngleCNN().to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "train_losses, val_losses = [], []\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for imgs, angles in train_loader:\n",
        "        imgs, angles = imgs.to(DEVICE), angles.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs).squeeze()\n",
        "        loss = criterion(outputs, angles)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for imgs, angles in val_loader:\n",
        "            imgs, angles = imgs.to(DEVICE), angles.to(DEVICE)\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, angles)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {math.sqrt(train_loss):.8f} | Val Loss: {math.sqrt(val_loss):.8f}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FNVQykHBCaY"
      },
      "source": [
        "Por último, y por pijotada, creamos un gráfico con MatPlotLib para poder ver como desciende el error tanto en entrenamiento como en validación. Una GUI asíncrona podríamos decir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsnsgfTwGorW"
      },
      "outputs": [],
      "source": [
        "# Visualizar pérdidas\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.savefig(\"model_loss.png\", bbox_inches='tight')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
